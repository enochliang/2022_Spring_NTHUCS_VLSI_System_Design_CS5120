{"cells":[{"cell_type":"markdown","metadata":{"id":"zTvIwDlYvBzC"},"source":["# HW1: LeNet-5 with Post-training Quantization\n","[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) is considered to be the first ConvNet.\n","We are going to implement a neural architecture similar to LeNet and train it with [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n","\n","Before we start, you may check this [Tensorspace-LeNet](https://tensorspace.org/html/playground/lenet.html) to play with LeNet and get familiar with this neural architecture.\n","\n","![image](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)\n","Ref.: LeCun et al., Gradient-Based Learning Applied to Document Recognition, 1998a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK5HtqxHcFhG"},"outputs":[],"source":["#!pip install torchinfo"]},{"cell_type":"markdown","metadata":{"id":"cb_RUhVeUwXx"},"source":["<font color='red'>Name: 梁謙行 Student ID: 110061644 </font>"]},{"cell_type":"markdown","metadata":{"id":"FZEOr8ulUwXy"},"source":["## 1. Initial Setup\n","\n","We are going to implement and train this neural network with PyTorch. \n","If you are not familiar with PyTorch, check the [official tutorail](https://pytorch.org/tutorials/beginner/basics/intro.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbiiMcdNJI--"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"nCaMDWYArEXO"},"source":["### 1.1 Loading dataset\n","Load training and test data from the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5UuOjjrnogR"},"outputs":[],"source":["transform = transforms.Compose(\n","    [\n","     transforms.Resize((32, 32)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","trainset = torchvision.datasets.MNIST(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.MNIST(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=8,\n","                                         shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"l62CkyIwtSOv"},"source":["### 1.2 Defining the Neural Network \n","Define a simple CNN that classifies MNIST images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fL3F-7Rntog"},"outputs":[],"source":["from nnutils import LeNet\n","net = LeNet().to(device)"]},{"cell_type":"markdown","metadata":{"id":"Izsj2yKoUwX2"},"source":["### 1.3 Question: Profiling the Neural Architecture by TorchInfo\n","Torchinfo provides information complementary to what is provided by print(your_model) in PyTorch, similar to Tensorflow's `model.summary()` API to view the visualization of the model, which is helpful while debugging your network. Check this [link](https://github.com/TylerYep/torchinfo#how-to-use) about how to use TorchInfo by `summary()` and fill in the TODO in the following cell. You should get the result similar to the table below:\n","\n","```\n","==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","LeNet                                    --                        --\n","...\n","...\n","==========================================================================================\n","Total params: ...\n","...\n","Estimated Total Size (MB): ...\n","==========================================================================================\n","```\n","\n","Ref.: https://github.com/TylerYep/torchinfo\n","\n","Please read *B. LeNet-5* in the [original paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) and answer the following questions in the report.\n","1. What is the type (convolution, pooling, fully-connected layer, etc.), input activation size, output activation size, and activation function (if any) of each layer?\n","2. What is the difference between this neural architecture and the LeNet-5 in the [original paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)?\n","3. Could we replace the 3rd conv, the conv in c5, with a fully connected layer?"]},{"cell_type":"markdown","metadata":{"id":"SmxDpBKKUwX3"},"source":["### 1.3 Answers\n","<font color='red'>Write your answers here.</font>\n","1. Information of each layer.  \n","   Layer 1:\n","       Type : Convolution layer\n","       Input activation size : 32*32\n","       Output activation size : 28*28\n","       No. of Features : 6\n","   Layer 2:\n","       Type : Subsampling layer\n","       Input activation size : 28*28\n","       Output activation size : 14*14\n","       No. of Features : 6\n","   Layer 3:\n","       Type : Convolution layer\n","       Input activation size : 14*14\n","       Output activation size : 10*10\n","       No. of Features : 16\n","   Layer 4:\n","       Type : Subsampling layer\n","       Input activation size : 10*10\n","       Output activation size : 5*5\n","       No. of Features : 16\n","   Layer 5:\n","       Type : Convolution layer\n","       Input activation size : 5*5\n","       Output activation size : 1*1\n","       No. of Features : 120\n","   Layer 6:\n","       Type : Fully Connected layer\n","       Input activation size : 5*5\n","       Output activation size : 1*1\n","       No. of Units : 84\n","   Layer 7:\n","       Type : Fully Connected layer\n","       Input activation size : 1*1\n","       Output activation size : 1*1\n","       No. of Units : 10\n","\n","2. 這個neural architecture 沒有bias\n","3. Yes, because convolution layer is a fully connected layer with some zero weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXMdhsckUwX4"},"outputs":[],"source":["from torchinfo import summary\n","# TODO\n","summary(net)"]},{"cell_type":"markdown","metadata":{"id":"Nijieuxptag6"},"source":["### 1.4 Training and Testing the Neural Network\n","Train this CNN on the training dataset (this may take a few moments).\n","* Check how to save and load the model\n","    * https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","    * Save:\n","        ```\n","        torch.save(model.state_dict(), PATH)\n","        ```\n","    * Load:\n","        ```\n","        model = TheModelClass(*args, **kwargs)\n","        model.load_state_dict(torch.load(PATH))\n","        model.eval()\n","        ```\n","* After training the model, we will save it as `lenet.pt`.\n","* You should comment out `train(net, trainloader, device)` and uncomment `net.load_state_dict(torch.load('lenet.pt'))` before submitting your homework.\n","    * Also, reloading the model from `lenet.pt` can save your time if there is something wrong and you need to restart and run all."]},{"cell_type":"markdown","metadata":{"id":"u0Ig9VkSUwX5"},"source":["If you get `ModuleNotFoundError: No module named 'nnutils'` when running the following cell, you should check if the `nnutils` folder exists. `nnutils` and `homework1.ipynb` should be placed in the same directory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzK6ohj5oNCT"},"outputs":[],"source":["from nnutils import train, test\n","\n","#train(net, trainloader, device)\n","net.load_state_dict(torch.load('lenet.pt'))\n","score = test(net, testloader, None, device)\n","\n","print('Accuracy of the network on the test images: {}%'.format(score))\n","torch.save(net.state_dict(), 'lenet.pt')"]},{"cell_type":"markdown","metadata":{"id":"SQZoEjBSveV8"},"source":["## 2. Post-training Quantization\n","### 2.1 Question: Visualizing Weights"]},{"cell_type":"markdown","metadata":{"id":"5qKRX7ply7I2"},"source":["We have flattened all vectors for you by `tensor.view(-1)`.\n","\n","Plot a histogram of each weight and show the total range and 3-sigma range for each weight. Fill in the TODO in the following cell.\n","\n","hint: `np.histogram()` and `plt.hist()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2h7zJ8m3GAF"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","conv1_weights = net.c1[0].weight.data.cpu().view(-1)\n","conv2_weights = net.c3[0].weight.data.cpu().view(-1)\n","conv3_weights = net.c5[0].weight.data.cpu().view(-1)\n","fc1_weights = net.f6[0].weight.data.cpu().view(-1)\n","fc2_weights = net.output[0].weight.data.cpu().view(-1)\n","\n","\n","# TODO\n","counts, bins = np.histogram(conv1_weights)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg1 = torch.mean(conv1_weights)\n","std1 = torch.std(conv1_weights)\n","plt.xlim(avg1-(3*std1),avg1+(3*std1))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"conv1_weights\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(conv2_weights)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg2 = torch.mean(conv2_weights)\n","std2 = torch.std(conv2_weights)\n","plt.xlim(avg2-(3*std2),avg2+(3*std2))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"conv2_weights\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(conv3_weights)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg3 = torch.mean(conv3_weights)\n","std3 = torch.std(conv3_weights)\n","plt.xlim(avg3-(3*std3),avg3+(3*std3))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"conv3_weights\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(fc1_weights)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg4 = torch.mean(fc1_weights)\n","std4 = torch.std(fc1_weights)\n","plt.xlim(avg4-(3*std4),avg4+(3*std4))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"fc1_weights\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(fc2_weights)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg5 = torch.mean(fc2_weights)\n","std5 = torch.std(fc2_weights)\n","plt.xlim(avg5-(3*std5),avg5+(3*std5))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"fc2_weights\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2hKjshaHD11m"},"source":["### 2.2 Question:  Quantizing Weights\n","Computation of convolution or fully-connected layer can be expressed as\n","$$W\\times I = O$$\n","where $W$ is the weight tensor, $I$ is the input tensor, and $O$ is the output tensor.\n","Let $n_w$ be the scaling factor. We have $$W_q\\times I =n_w W \\times I\\approx n_w O$$ where $W_q$ is the quantized 8-bit signed integer weight tensor.\n","\n","Fill in the TODO in `quantized_weights()` of `quantize_layer_weights()`.If you’ve done everything correctly, the accuracy degradation should be negligible.\n","1. What is $n_w$? Explain how you get it.\n","2. What is the accuracy degradation? \\\n","Show both relative error and absolute error when the true value is the accuracy we get before performing any quantization."]},{"cell_type":"markdown","metadata":{"id":"l82xE4sqUwYG"},"source":["### 2.3 Answers\n","<font color='red'>Write your answers here.</font>\n","1. nw代表一個layer的scaling_factor將W矩陣乘上nw再取round down就代表對weights做quantization。\n","2. 原本的model在test中取得98.66%的accuracy,做了weight quantization後就變成98.45%下降了0.21%。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jN3iH5OtUwYH"},"outputs":[],"source":["score = test(net, testloader, None, device)\n","print('Accuracy of the network on the test images: {}%'.format(score))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXNk1fXuPGjB"},"outputs":[],"source":["from nnutils import copy_model, quantize_layer_weights\n","net_q2 = copy_model(net)\n","quantize_layer_weights(net_q2, device)\n","\n","score = test(net_q2, testloader, None, device)\n","print('Accuracy of the network after quantizing all weights: {}%'.format(score))"]},{"cell_type":"markdown","metadata":{"id":"xg7bfTF1bBVe"},"source":["### 2.3 Question: Visualizing Activations\n","Plot histograms of the input images and the output activations of each operation and answer the following questions in the report. Fill in the TODO in the following cell.\n","1. Discuss any observations about the distribution of these activations.\n","2. Record the range of the values, as well as their 3-sigma range (the difference between μ + 3σ and μ − 3σ)."]},{"cell_type":"markdown","metadata":{"id":"54HPL9VfUwYJ"},"source":["### 2.3 Answers\n","<font color='red'>Write your answers here.</font>\n","1. 各個layer output因為經過ReLU所以在零上面的分布會特別高。\n","2. 以下繪圖以正負μ + 3σ and μ − 3σ為邊界，圖片如下面程式結果。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3MLymKyUwYJ"},"outputs":[],"source":["net_q3 = copy_model(net)\n","\n","def visualizeActivations(module, input, output):\n","    if module.profile_activations == True:\n","        module.inAct = input[0].cpu().view(-1)\n","        module.outAct = output[0].cpu().view(-1)\n","    \n","for name, model in net_q3.named_children():\n","    model.profile_activations = True\n","    model.register_forward_hook(visualizeActivations)\n","net_q3.eval()\n","with torch.no_grad():\n","    input = trainset[0][0].unsqueeze(0)\n","    _ = net_q3(input.to(device))  \n","for name, model in net_q3.named_children(): model.profile_activations = False "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEo8VK46bwjn"},"outputs":[],"source":["input_activations = net_q3.c1.inAct\n","c1_output_activations = net_q3.c1.outAct\n","c3_output_activations = net_q3.c3.outAct\n","c5_output_activations = net_q3.c5.outAct\n","f6_output_activations = net_q3.f6.outAct\n","output_output_activations = net_q3.output.outAct\n","\n","# TODO\n","counts, bins = np.histogram(input_activations)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg0 = torch.mean(input_activations)\n","std0 = torch.std(input_activations)\n","plt.xlim(avg0-(3*std0),avg0+(3*std0))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"input_activations\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(c1_output_activations)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg1 = torch.mean(c1_output_activations)\n","std1 = torch.std(c1_output_activations)\n","plt.xlim(avg1-(3*std1),avg1+(3*std1))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"c1_output_activations\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(c3_output_activations)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg2 = torch.mean(c3_output_activations)\n","std2 = torch.std(c3_output_activations)\n","plt.xlim(avg2-(3*std2),avg2+(3*std2))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"c3_output_activations\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(c5_output_activations)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg3 = torch.mean(c5_output_activations)\n","std3 = torch.std(c5_output_activations)\n","plt.xlim(avg3-(3*std3),avg3+(3*std3))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"c5_output_activations\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(f6_output_activations)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg4 = torch.mean(f6_output_activations)\n","std4 = torch.std(f6_output_activations)\n","plt.xlim(avg4-(3*std4),avg4+(3*std4))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"f6_output_activations\")\n","plt.show()\n","print('\\n')\n","\n","counts, bins = np.histogram(output_output_activations)\n","n, bins, patches = plt.hist(bins[:-1], bins, weights=counts)\n","avg5 = torch.mean(output_output_activations)\n","std5 = torch.std(output_output_activations)\n","plt.xlim(avg5-(3*std5),avg5+(3*std5))\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Count\")\n","plt.title(\"output_output_activations\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"haiPVx4ibEra"},"source":["### 2.4 Question:  Quantizing Activations\n","The output of conv in `c1` can be $$W_{conv1}* I=O_{conv1}.$$\n","Let the scaling factor of the quantized input matrix $I$ be $n_I$, the scaling factor of the quantized weight matrix $W_{conv1}$ \n","be $n_{W_{c1}}$, and the scaling factor of the output matrix $O_{conv1}$ be $n_{O_{c1}}$.  \n","$$W_{conv1_q}* I_q = (n_{W_{c1}}W_{conv1})* (n_II)\\approx (n_{W_{c1}}n_I)O_{conv1}$$\n","where $W_{conv1_q}$ is the quantized 8-bit signed integer weight tensor and $I_q$ is the quantized 8-bit signed integer input activation tensor.\n","\n","On the other hand,\n","$$O_{conv1_q} \\approx n_{O_{c1}}O_{conv1}$$\n","where $O_{conv1_q}$ is the quantized 8-bit signed integer output activation tensor \"without\" considering quantized weight tensor and quantized input activation tensor.\n","\n","Since we're doing post-training quantization, we can get $n_I$, $n_{W_{c1}}$, and $n_{O_{c1}}$ first and do the other calculations for quantization.\n","\n","Answer the following questions in the report.\n","\n","1. How to compute $n_I$, $n_{W_{c1}}$, and $n_{O_{c1}}$? \n","2. The ture quantized output activation tensor is depend on $W_{conv1_q}$ and $I_q$, so we cannot simply apply $n_{O_{c1}}$ on the output of $W_{conv1_q}* I_q$ to quantize the output activation. Derive an equation for the quantized output of the conv in c1 after quantizing activation and weight with  $n_I$, $n_{W_{c1}}$, and $n_{O_{c1}}$ and show the scaling factor $S_1$ of it. \\\n","(hint: quantize $O$ in $W_{conv1_q}* I_q = O$ and get $W_{conv1_q}* I_q \\approx S_1O = O_q$ where $O_q$ is the quantized 8-bit signed integer output.)\n","3. Derive an equation for the quantized output of the conv in `c3` after quantizing activation and weight.\n","4. Show the general form of the equation for layer $l$ to calculate the scaling factor $S_l$ of output activation.\n","    * You may use notations like $W_l, O_l$ to indicate the weight and output activation of layer $l$, respectively.\n","    * Fill in the TODO in `quantize_initial_input()` and `quantize_activations()` of `NetQuantized()` to compute $n_I$ and $S_l$ for layer $l$ that scale values to 8-bit signed integer.\n","5. As for `forward()` of `NetQuantized()`, make sure you use fixed-point representation when doing any calculation with input/output scale. Keep in mind that we will implement a hardware accelerator with this model, so it is better not to do any floating-point computation.\n","    * You will have to fill in the TODO in `forward()` to scale the initial input and scale the outputs of each layer. Please follow these steps whenever processing with input/output scale: \n","        1. `scale = round(scale*(2**16))`: Now we get a fixed-point number, and we will save this value to scale_hw.json later\n","        2. `(scale*features) >> 16`: `>>` means right bit shift operator \n","        3. Clamp the value between -128 and 127\n","    \n","    * If you have done everything correctly, the accuracy degradation should be negligible. \n","    * What is the accuracy degradation? Show both relative error and absolute error when the true value is the accuracy we get before performing any quantization.\n","6. What are the pros and cons of using fixed-point representation when forwarding feature maps to the next layer?\n","7. Show the input/output scale before and after `round(scale*(2**16))`?"]},{"cell_type":"markdown","metadata":{"id":"y5oJ5oDkUwYK"},"source":["### 2.4 Answers\n","<font color='red'>Write your answers here.</font>\n","1. $n_{i}$是由最初的 input_activation 去取最大絕對值將他乘以二，用255除以這個值算出 initial_input 的 scaling_factor。\n","\n","   $n_{wc1}$是由c1那層的所有weights中取最大絕對值將他乘以二，用255除以這個值算出 c1 weights 的scaling_factor。\n","\n","   $n_{O_{c1}}$是由 c1 那層的 output activations 中取最大絕對值將他乘以二，用255除以這個值算出 c1 output 的 scaling factor。\n","\n","2. $O$ = $W_{conv1_q}*I$\n","   \n","3. $O_{c3} = W_{c3}*W_{c1}*I$\n","\n","   $O_{q_{c3}} = O*n_{o_{c3}} = S_{c3}(W_{c3}*n_{c3})*S_{c1}(W_{c1}*n_{c1})(I*n_I)$\n","\n","   乘上$S_{c3}$再取floor就完成了c3後的activation quantization。\n","4. $S_l$ = $n_{o_l} / [ n_{o_{(l-1)}} * n_{w_l} ]$\n","\n","    $n_{o_0}$ = $n_I$\n","5. 請參考quantutils.py NetQuantized()\n","6. 優點:節省能量，降低硬體複雜度，可能可以提升速度。\n","\n","   缺點:會產生quantization error。\n","7. $Before:$\n","   \n","    \"input_scale\": 127.5,\n","\n","    \"c1_output_scale\": 0.0014214414404705167,\n","\n","    \"c3_output_scale\": 0.003633452346548438,\n","\n","    \"c5_output_scale\": 0.002333736279979348,\n","\n","    \"f6_output_scale\": 0.005027502775192261,\n","\n","    \"output_output_scale\": 0.0038048995193094015\n","\n","    $After:$\n","\n","    \"input_scale\": 128,\n","    \n","    \"c1_output_scale\": 93,\n","    \n","    \"c3_output_scale\": 238,\n","    \n","    \"c5_output_scale\": 153,\n","    \n","    \"f6_output_scale\": 329,\n","    \n","    \"output_output_scale\": 249"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13CpHgvE994J"},"outputs":[],"source":["from nnutils import NetQuantized\n","from copy import deepcopy\n","\n","net_init = copy_model(net_q2)\n","net_init.input_activations = deepcopy(net_q3.c1.inAct)\n","        \n","for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n","    layer_init.inAct = deepcopy(layer_q3.inAct)\n","    layer_init.outAct = deepcopy(layer_q3.outAct)\n","\n","net_quantized = NetQuantized(net_init)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcBXEodN6hrY"},"outputs":[],"source":["score = test(net_quantized, testloader, max_samples=None, device=device)\n","print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"]},{"cell_type":"markdown","metadata":{"id":"1jTOL7scbMs7"},"source":["### 2.5 Question:  Quantizing Biases\n","We add a bias in the final layer of this LeNet.\n","\n","Answer the following questions in the report.\n","1. Now, the equation is $$W*I+\\beta = O,$$ where $\\beta$ is the bias. Derive the equation of a quantized layer with bias.\\\n","Note that our biases are commonly quantized to 32-bits. Therefore, your bias values are not necessary between -128 and 127.\n","2. What is the scaling factor for the bias?\\\n","(hint: the form looks just like what we have done for quantizing activations)\n","3. Fill in the TODO in `NetQuantizedWithBias()`.If you have done everything correctly, the accuracy degradation should be negligible. What is the accuracy degradation?\n","    * Show both relative error and absolute error when the true value is the accuracy we get before performing any quantization."]},{"cell_type":"markdown","metadata":{"id":"fSLQfudTUwYM"},"source":["### 2.5 Answers\n","<font color='red'>Write your answers here.</font>\n","1. $W*I+\\beta = O$\n","\n","   $S_1(W*n_w)(I*n_I)+\\beta(n_wn_IS_1) = On_o = O_q$\n","2. $n_b$ = $ n_{I} * S_{c1} * S_{c3} * S_{c5} * S_{f6} * S_{output} * n_{w_{c1}} * n_{w_{c3}} * n_{w_{c5}} * n_{w_{f6}} * n_{w_{output}}$\n","3. Code請參考quantutils.py NetQuantizedWithBias()。accuracy從97.67%降至97.63%，下降0.04%。"]},{"cell_type":"markdown","metadata":{"id":"0H3uhCZyUwYM"},"source":["* After training the model, we will save it as `lenet_with_bias.pt`.\n","* You should comment out `train(net_with_bias, trainloader, device)` and uncomment `net_with_bias.load_state_dict(torch.load('lenet_with_bias.pt'))` before submitting your homework.\n","    * Note that reloading the model from `lenet_with_bias.pt` can save your time. You don't need train the model from scratch every time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjk3hEQaVDpq"},"outputs":[],"source":["from nnutils import LeNet_with_bias\n","\n","net_with_bias = LeNet_with_bias().to(device)\n","#train(net_with_bias, trainloader, device)\n","net_with_bias.load_state_dict(torch.load('lenet_with_bias.pt'))\n","score = test(net_with_bias, testloader, max_samples=None, device=device)\n","\n","print('Accuracy of the network (with a bias) on the test images: {}%'.format(score))\n","torch.save(net_with_bias.state_dict(), 'lenet_with_bias.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_ZiJk6yEEM-","scrolled":true},"outputs":[],"source":["for name, model in net_with_bias.named_children():\n","    model.profile_activations = True\n","    model.register_forward_hook(visualizeActivations)\n","net_with_bias.eval()\n","with torch.no_grad():\n","    input = trainset[0][0].unsqueeze(0)\n","    _ = net_with_bias(input.to(device))\n","for name, model in net_with_bias.named_children(): model.profile_activations = False "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZwk8KLtAUAM"},"outputs":[],"source":["net_with_bias_with_quantized_weights = copy_model(net_with_bias)\n","quantize_layer_weights(net_with_bias_with_quantized_weights, device)\n","\n","score = test(net_with_bias_with_quantized_weights, testloader, max_samples=None, device=device)\n","print('Accuracy of the network on the test images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mO2Gdu_tEZ4v"},"outputs":[],"source":["from nnutils import NetQuantizedWithBias\n","net_quantized_with_bias = NetQuantizedWithBias(net_with_bias_with_quantized_weights)\n","score = test(net_quantized_with_bias, testloader, max_samples=None, device=device)\n","print('Accuracy of the network on the test images after all the weights and the bias are quantized: {}%'.format(score))"]},{"cell_type":"markdown","metadata":{"id":"U_W-RCmHjlkg"},"source":["# Extract the inputs and outputs of the quantized model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AllumB6-YI9B"},"outputs":[],"source":["# Replace and save the model with quantized biases.\n","inference_model = copy_model(net_quantized_with_bias)\n","torch.save(inference_model.state_dict(), 'net_quantized_with_bias.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWPqoj8tf0Rr"},"outputs":[],"source":["# Use the accuray to check if it remains the same.\n","score = test(inference_model, testloader, max_samples=None, device = device)\n","print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"]},{"cell_type":"markdown","metadata":{"id":"5TfpcnC60_g4"},"source":["Choose 100 images to generate patterns for our homework 2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CByZgw-ftMBD"},"outputs":[],"source":["index = range(100)"]},{"cell_type":"markdown","metadata":{"id":"x9M4QXt60WmL"},"source":["Save the input/output activations to the CSV format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSIdYFyZshFj"},"outputs":[],"source":["import os \n","import zipfile\n","# It is easier to download all the files zipped.\n","zf = zipfile.ZipFile('parameters.zip', 'w', zipfile.ZIP_DEFLATED)\n","\n","if not os.path.exists('./activations'):\n","    os.mkdir('./activations')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HDjYxGuUwYP"},"outputs":[],"source":["for ind in range(100):\n","    if not os.path.exists('./activations/img{}'.format(ind)):\n","        os.mkdir('./activations/img{}'.format(ind))\n","\n","    for name, model in inference_model.named_children():\n","        model.profile_activations = True\n","        model.register_forward_hook(visualizeActivations)\n","    input, label = testset[index[ind]]\n","    output = inference_model(input.unsqueeze(0).to(device))\n","    for name, model in inference_model.named_children(): model.profile_activations = False \n","    \n","    np.savetxt('./activations/img{}/input.csv'.format(ind), input.cpu().data.numpy().reshape(-1), delimiter=',')\n","    np.savetxt('./activations/img{}/output.csv'.format(ind), output.cpu().data.numpy().reshape(-1).astype(int), delimiter=',')\n","    zf.write('./activations/img{}/input.csv'.format(ind))\n","    zf.write('./activations/img{}/output.csv'.format(ind))\n","    \n","    opDict = {\n","        'c1': (inference_model.c1.inAct, inference_model.c1.outAct),\n","        's2': (inference_model.s2.inAct, inference_model.s2.outAct),\n","        'c3': (inference_model.c3.inAct, inference_model.c3.outAct),\n","        's4': (inference_model.s4.inAct, inference_model.s4.outAct),\n","        'c5': (inference_model.c5.inAct, inference_model.c5.outAct),\n","        'f6': (inference_model.f6.inAct, inference_model.f6.outAct),\n","        'output': (inference_model.output.inAct, inference_model.output.outAct)\n","    }\n","    \n","    for key in opDict:\n","        if not os.path.exists('./activations/img{}/{}'.format(ind, key)):\n","            os.mkdir('./activations/img{}/{}'.format(ind, key))\n","        np.savetxt('./activations/img{}/{}/input.csv'.format(ind, key), opDict[key][0].cpu().data.numpy().reshape(-1).astype(int), delimiter=',')\n","        np.savetxt('./activations/img{}/{}/output.csv'.format(ind, key), opDict[key][1].cpu().data.numpy().reshape(-1).astype(int), delimiter=',')\n","        zf.write('./activations/img{}/{}/input.csv'.format(ind, key))\n","        zf.write('./activations/img{}/{}/output.csv'.format(ind, key))"]},{"cell_type":"markdown","metadata":{"id":"5jo_TlzE1dqy"},"source":["Save the weights of each layer to the CSV format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27MD_hJUzlnf"},"outputs":[],"source":["if not os.path.exists('./weights'):\n","    os.mkdir('./weights')\n","    \n","for name, weights in inference_model.state_dict().items():\n","    print(name, 'with shape:' , weights.shape)\n","    np.savetxt('./weights/%s.csv' %(name) , weights.cpu().numpy().reshape(-1).astype(int), delimiter=',')\n","    zf.write('./weights/%s.csv' %(name))"]},{"cell_type":"markdown","metadata":{"id":"mG9TAca-Jbg6"},"source":["Record the scaling factors to JSON file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNqV21YVimKD"},"outputs":[],"source":["import json\n","scales = {'input_scale':inference_model.input_scale.item(), 'c1_output_scale': inference_model.c1.output_scale.item(), 'c3_output_scale':inference_model.c3.output_scale.item(),\n","        'c5_output_scale' :inference_model.c5.output_scale.item(), 'f6_output_scale':inference_model.f6.output_scale.item(), 'output_output_scale':inference_model.output.output_scale.item()}\n","print(scales)\n","with open('scale.json', 'w', newline='') as jsonfile:\n","    json.dump(scales, jsonfile)\n","\n","zf.write('./scale.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GG9P9rjbUwYQ"},"outputs":[],"source":["import json\n","scale_hw = {\n","    'input_scale': int(round(inference_model.input_scale.item())), \n","    'c1_output_scale': int(round(inference_model.c1.output_scale.item()*(2**16))), \n","    'c3_output_scale': int(round(inference_model.c3.output_scale.item()*(2**16))),\n","    'c5_output_scale' : int(round(inference_model.c5.output_scale.item()*(2**16))), \n","    'f6_output_scale': int(round(inference_model.f6.output_scale.item()*(2**16))), \n","    'output_output_scale': int(round(inference_model.output.output_scale.item()*(2**16)))\n","}\n","print(scale_hw)\n","with open('scale_hw.json', 'w', newline='') as jsonfile:\n","    json.dump(scale_hw, jsonfile)\n","\n","zf.write('./scale_hw.json')"]},{"cell_type":"markdown","metadata":{"id":"HbUYA1VpKLAB"},"source":["Save the zip file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlOLRLdTJqcm"},"outputs":[],"source":["zf.close()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"homework1.ipynb","provenance":[{"file_id":"12ETP3DfjvmL3C_J3MzCYQO6kx63xchet","timestamp":1612245741926}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
